{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a non-parametric, instance-based learning algorithm widely used for classification and regression. There are different variations of KNN based on specific purposes and improvements:\n",
    "\n",
    "1. Basic KNN\n",
    "Concept: It finds the \"K\" closest data points to a query point based on a chosen distance metric (e.g., Euclidean, Manhattan).\n",
    "Application: Used for classification or regression tasks.\n",
    "2. Weighted KNN\n",
    "Concept: Assigns weights to neighbors based on their distance. Closer points have a higher influence than farther points.\n",
    "Example: Weight = \n",
    "1\n",
    "ùëë\n",
    "ùëñ\n",
    "ùë†\n",
    "ùë°\n",
    "ùëé\n",
    "ùëõ\n",
    "ùëê\n",
    "ùëí\n",
    "distance\n",
    "1\n",
    "‚Äã\n",
    " \n",
    "Application: Handles cases where nearby neighbors are more important.\n",
    "3. KNN with Different Distance Metrics\n",
    "Concept: Distance metrics can be modified to suit data:\n",
    "Euclidean Distance: \n",
    "\n",
    "<img src= \"knn.png\">\n",
    "Minkowski Distance: A generalization that combines Euclidean and Manhattan.\n",
    "Hamming Distance: For categorical data.\n",
    "Application: Helps in optimizing KNN for different data types.\n",
    "4. KNN for Classification\n",
    "Concept: Majority voting among the K-nearest neighbors decides the class label.\n",
    "Application: Image classification, text classification, etc.\n",
    "5. KNN for Regression\n",
    "Concept: The output is the average (or weighted average) of the target values of the K-nearest neighbors.\n",
    "Application: Predicting continuous outputs, such as house prices.\n",
    "6. K-D Tree KNN\n",
    "Concept: Uses a K-Dimensional Tree to organize data for faster nearest-neighbor searches.\n",
    "Benefit: Speeds up searching in higher-dimensional data compared to brute force.\n",
    "Application: Large datasets with multiple features.\n",
    "7. Ball Tree KNN\n",
    "Concept: Partitions data using hyperspheres (balls) instead of axes (like K-D Tree).\n",
    "Benefit: Better for datasets with many dimensions.\n",
    "Application: High-dimensional nearest neighbor search.\n",
    "8. Fuzzy KNN\n",
    "Concept: Assigns soft probabilities for class membership instead of hard classification based on votes.\n",
    "Application: Situations where overlap exists between classes.\n",
    "9. Distance-Weighted KNN for Regression\n",
    "Concept: Uses distance-weighted average for predictions in regression tasks.\n",
    "Application: Improves accuracy for non-uniformly distributed data.\n",
    "10. Approximate Nearest Neighbors (ANN)\n",
    "Concept: Reduces computation by approximating nearest neighbors using techniques like hashing.\n",
    "Application: Real-time search tasks (e.g., recommendation systems).\n",
    "Summary\n",
    "KNN can be adapted in various ways to improve efficiency, accuracy, and versatility for specific tasks. Variations like weighted KNN, K-D trees, and ANN help overcome its basic limitations like slow searching or poor performance in high dimensions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
